##  Lecture 6: Self-Attention in Transformers

- Adding an attention mechanism to RNNs
  - Attention helps RNNs with accessing information
  - The original attention mechanism for RNNs
  - Processing the inputs using a bidirectional RNN
  - Generating outputs from context vectors
  - Computing the attention weights
- Introducing the self-attention mechanism
  - Starting with a basic form of self-attention
  - Parameterizing the self-attention mechanism: scaled dot-product attention
- Attention is all we need: introducing the original transformer architecture
  - Encoding context embeddings via multi-head attention
  - Learning a language model: decoder and masked multi-head attention
  - Implementation details: positional encodings and layer normalization
