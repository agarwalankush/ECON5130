{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCCEsgjrn-7b"
   },
   "source": [
    "**Outline**\n",
    "- [Implementing RNNs for sequence modeling in PyTorch](#Implementing-RNNs-for-sequence-modeling-in-PyTorch)\n",
    "  - [Predicting the sentiment of IMDb movie reviews](#Predicting-the-sentiment-of-IMDb-movie-reviews)\n",
    "  - [Preparing the movie review data](#Preparing-the-movie-review-data)\n",
    "  - [Embedding layers for sentence encoding](#Embedding-layers-for-sentence-encoding)\n",
    "  - [Building an RNN model](#Building-an-RNN-model)\n",
    "  - [Building an RNN model for the sentiment analysis task](#Building-an-RNN-model-for-the-sentiment-analysis-task)\n",
    "  - [The bidirectional RNN](#The-bidirectional-RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am9cW1u1n-7d"
   },
   "source": [
    "# Implementing RNNs for sequence modeling in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxWH-63vn-7e"
   },
   "source": [
    "## Predicting the sentiment of IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WLN_S2sn-7f"
   },
   "source": [
    "The sentiment analysis (language modeling) is concerned with analyzing the expressed opinion of a sentence or a text document. This can be very useful in creating a sentiment score for an exchange-listed firm or even a financial index by creating a sentiment score based on news articles and analyst reports. This is typically an approach which is used to automate the economicy policy sentiment indices such as the one developed by Baker and Bloom: https://www.policyuncertainty.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wCw0RCqn-7g"
   },
   "source": [
    "As any sentiment score would be based on a long sequence of text inputs, we implement a many-to-many RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lU-eT0sn-7g"
   },
   "source": [
    "### Preparing the movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8l98g5A2n-7h"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-EUGpB8n-7j"
   },
   "source": [
    "Each set has 25,000 samples. And each sample of the datasets consists of two elements, the sentiment label representing the target label we want to predict (`neg` refers to negative sentiment and `pos` refers to positive sentiment), and the movie review text (the input features). The text component of these movie reviews is sequences of words, and the RNN model classifies each sequence as a positive (`1`) or negative (`0`) review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUKac7gGn-7k"
   },
   "source": [
    "Before we feed the data into an RNN model, we need to apply several preprocessing steps:\n",
    "1. Split the training dataset into separate training and validation partitions.\n",
    "2. Identify the unique words in the training dataset\n",
    "3. Map each unique word to a unique integer and encode the review text into encoded integers\n",
    "(an index of each unique word)\n",
    "4. Divide the dataset into mini-batches as input to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWCJmVKWn-7k"
   },
   "source": [
    "First, we import the necessary modules and read the data from `torchtext` (which we install via `pip install torchtext`). To import data, we need to install `torchdata` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnz9DY4EpEf_",
    "outputId": "2ce33a2d-9d16-449d-b96b-f52bcba1417e"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE_1BYiTpNiP",
    "outputId": "ef53f192-f4a3-4d32-ec8a-36bf6dd8a91e"
   },
   "outputs": [],
   "source": [
    "!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq2uV1-hpRGI",
    "outputId": "aeba28d0-516b-4164-a8af-ccff17a77c9b"
   },
   "outputs": [],
   "source": [
    "!pip install --user \"git+https://github.com/pytorch/data.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrPOyvCqqr8s",
    "outputId": "85cda31f-53f5-42ab-f48d-c1badd878f28"
   },
   "outputs": [],
   "source": [
    "!pip install -U torchtext==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, in the menu, go to **Runtime** and **Restart Runtime**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "3Hj3Nd9bn-7l",
    "outputId": "4a176623-5a10-422e-d0ee-de18e0badd61"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Step 1: load and create the datasets\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C54O9bC4vpXx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk43EiZqn-7m"
   },
   "source": [
    "The original training dataset contains 25,000 examples. 20,000 examples are randomly chosen for training, and 5,000 for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSMi_LcUn-7n"
   },
   "source": [
    "To prepare the data for input to an NN, we need to encode it into numeric values. To do this, we first find the unique words (tokens) in the training dataset. It is more efficient to use the `Counter` class from the `collections` package, which is part of Python’s standard library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgWyU8xCn-7o"
   },
   "source": [
    "In the following code, we instantiate a new `Counter` object (`token_counts`) that collects the unique word frequencies. Note that in this particular application (and in contrast to the `bag-of-words` model), we are only interested in the set of unique words and won’t require the word counts, which are created as a side product. In the `bag-of-words` model, we would have required to keep track of word count as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_oqYiptn-7o"
   },
   "source": [
    "The `tokenizer` function removes HTML markups as well as punctuation and other non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOgQsrySn-7p",
    "outputId": "1e317d01-a14b-4e08-af4e-3fe20b454e68"
   },
   "outputs": [],
   "source": [
    "## Step 2: find unique tokens (words)\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApM5RYyLn-7r"
   },
   "source": [
    "We map each unique word to a unique integer. This can be done manually using a Python dictionary, where the keys are the unique tokens (words) and the value associated with each key is a unique integer. However, the `torchtext` package already provides a class, `Vocab`, which we use to create such a mapping and encode the entire dataset. \n",
    "\n",
    "First, we create a `vocab` object by passing the ordered dictionary mapping tokens to their corresponding occurrence frequencies\n",
    "(the ordered dictionary is the sorted `token_counts`). Second, we prepend two special tokens to the vocabulary – the padding and the unknown token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O3Vr4yin-7s"
   },
   "source": [
    "To demonstrate how to use the `vocab` object, we convert an example input text into a list of integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGvkgJlyn-7t",
    "outputId": "04d7470d-311a-40cb-9c75-295f22c3f9b0"
   },
   "outputs": [],
   "source": [
    "## Step 3: encoding each unique token into integers\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print([vocab[token] for token in ['bad', 'movie', 'terrible', 'watch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z44xvRzVn-7u"
   },
   "source": [
    "Note that there might be some tokens in the validation or testing data that are not present in the training\n",
    "data and are thus not included in the mapping. If we have $q$ tokens (that is, the size of `token_counts`\n",
    "passed to `Vocab`, which in this case is 69,023), then all tokens that haven’t been seen before, and are\n",
    "thus not included in `token_counts`, will be assigned the integer `1` (a placeholder for the unknown token).\n",
    "In other words, the index `1` is reserved for unknown words. Another reserved value is the integer\n",
    "`0`, which serves as a placeholder, a so-called padding token, for adjusting the sequence length. This is handy when we are building an RNN model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_zlrR-zn-7v"
   },
   "source": [
    "Next we define the `text_pipeline` function to transform each text in the dataset accordingly and the `label_pipeline` function to convert each label to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uft7zaOTn-7w"
   },
   "outputs": [],
   "source": [
    "## Step 3-A: define the functions for transformation\n",
    "\n",
    "# device = torch.device(\"cuda:0\") # use this when GPU on the computing device\n",
    "device = 'cpu'\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUwpi-Ccn-7w"
   },
   "source": [
    "We generate batches of samples using `DataLoader` and pass the data processing pipelines declared previously to the argument `collate_fn`. We wrap the text encoding and label transformation function into the `collate_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvEd_luQn-7w"
   },
   "outputs": [],
   "source": [
    "## Step 3-B: wrap the encode and transformation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), \n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bybBX7MYn-7x"
   },
   "source": [
    "Above we convert sequences of words into sequences of integers, and labels of `pos` or `neg` into 1 or 0. However, the sequences currently have different lengths (as shown in the result of executing the following code for four examples). Although, in general, RNNs can handle sequences with different lengths, we still need to make sure that all the sequences in a mini-batch have the same length to store them efficiently in a tensor.\n",
    "\n",
    "PyTorch provides an efficient method, `pad_sequence()`, which automatically pads the consecutive elements that are to be combined into a batch with placeholder values (0s) so that all sequences within a batch will have the same shape. \n",
    "\n",
    "In the previous code, we already created a data loader of a small batch size from the training dataset and applied the `collate_batch` function, which itself included a `pad_sequence()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yXAeNwfn-7x",
    "outputId": "bed5ab40-df44-468b-99ee-3f2d6cacf01b"
   },
   "outputs": [],
   "source": [
    "## Take a small batch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qsJgNkvn-7y"
   },
   "source": [
    "As we can see above, after the padding has been applied, the batch has all 4 examples of length 218."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BICKNTLn-7y"
   },
   "source": [
    "Finally, we divide all three datasets into data loaders with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGo4u4stn-7z"
   },
   "outputs": [],
   "source": [
    "## Step 4: batching the datasets\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch4UDyz7n-7z"
   },
   "source": [
    "Now, the data is in a suitable format for an RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdqpBdSin-7z"
   },
   "source": [
    "We first discuss feature **embedding**, which is an optional but highly recommended preprocessing step that is used to reduce the dimensionality of the word vectors. Otherwise the input dimension could be unnecessarily large (think of dimension being the length of the input examples vocabulary), which can slow down the training or even result in a bad model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86wSrEhsn-70"
   },
   "source": [
    "### Embedding layers for sentence encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtpP_hkDn-70"
   },
   "source": [
    "During the data preparation in the previous step, we generated sequences of the same length. The elements of these sequences were integer numbers that corresponded to the indices of unique words. These word indices can be converted into input features in several different ways. \n",
    "\n",
    "One naive way is to apply one-hot encoding to convert the indices into vectors of zeros and ones. Then, each word will\n",
    "be mapped to a vector whose size is the number of unique words in the entire dataset. Given that the number of unique words (the size of the vocabulary) can be in the order of $10^4$ – $10^5$, which will also be the number of our input features, a model trained on such features may suffer from the **curse of dimensionality**. Furthermore, these features are very sparse since all are zero except one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJDZ4fBFn-70"
   },
   "source": [
    "A more elegant approach is to map each word to a vector of a fixed size with real-valued elements (not necessarily integers). In contrast to the one-hot encoded vectors, we can use finite-sized vectors to represent an infinite number of real numbers. (In theory, we can extract infinite real numbers from a given interval, for example [–1, 1].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z802Fvxgn-71"
   },
   "source": [
    "This is the idea behind embedding, which is a feature-learning technique that we utilize here to automatically learn the salient features to represent the words in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U4pZx72n-71"
   },
   "source": [
    "Given the number of unique words, $n_{words}$, we can select the size of the embedding vectors (a.k.a., embedding dimension)\n",
    "to be much smaller than the number of unique words ($embedding\\_dim << n_{words}$) to represent the entire vocabulary as input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vH5juPHn-72"
   },
   "source": [
    "The advantages of embedding over one-hot encoding are as follows:\n",
    "- A reduction in the dimensionality of the feature space to decrease the effect of the curse of dimensionality\n",
    "- The extraction of salient features since the embedding layer in an NN can be optimized (or learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWBDr3Spn-72"
   },
   "source": [
    "Given a set of tokens of size $n_{words} + 2$ ($n_{words}$) is the size of the token set, plus index 0 is reserved for the padding placeholder, and 1 is for the words not present in the token set), an embedding matrix of size\n",
    "$(n_{words} + 2) \\times embedding\\_dim$ will be created where each row of this matrix represents numeric features\n",
    "associated with a token. Therefore, when an integer index, $i$, is given as input to the embedding, it\n",
    "will look up the corresponding row of the matrix at index $i$ and return the numeric features. \n",
    "\n",
    "The embedding matrix serves as the input layer to the NN models. In practice, creating an embedding layer can simply be done using `nn.Embedding`. Let’s see an example where we create an embedding layer and apply it to a batch of two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oQD3xJIn-73",
    "outputId": "1f32627f-0326-406e-d366-3d161e447661"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=10, \n",
    "                         embedding_dim=3, \n",
    "                         padding_idx=0)\n",
    " \n",
    "# a batch of 2 samples of 5 indices each\n",
    "text_encoded_input = torch.LongTensor([[1,2,4,5,6],[4,3,2,6,0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEeun754n-73"
   },
   "source": [
    "The input to this model (embedding layer) must have rank 2 with the dimensionality $batchsize \\times input\\_length$, where $input\\_length$ is the length of sequences (here, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb32bTH6n-74"
   },
   "source": [
    "For example, an input sequence in the mini-batch could be $<1, 8, 5, 9, 2>$, where each element of this sequence is the index of the unique words. The output will have the dimensionality $batchsize \\times input\\_length \\times embedding\\_dim$, where $embedding\\_dim$ is the size of the embedding features (here, set to 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzOaYKI-n-74"
   },
   "source": [
    "The other argument provided to the embedding layer, `num_embeddings`, corresponds to the unique integer values that the model will receive as input (for instance, $n + 2$, set here to 10). Therefore, the embedding matrix in this case has the size $10 \\times 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-x-_r1bn-75"
   },
   "source": [
    "`padding_idx` indicates the token index for padding (here, 0), which, if specified, will not contribute to the gradient updates during training. In our example, the length of the original sequence of the second sample is 4, and we padded it with 1 more element 0. The embedding output of the padded element is [0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye6QiQjYn-75"
   },
   "source": [
    "### Building an RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MLV9x4gn-75"
   },
   "source": [
    "Now we are ready to build an RNN model. Using the `nn.Module` class, we can combine the embedding layer, the recurrent layers of the RNN, and the fully connected non-recurrent layers. For the recurrent layers, we can use any of the following implementations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xjuNo2Wn-76"
   },
   "source": [
    "\n",
    "* **RNN layers:**\n",
    "  * `nn.RNN(input_size, hidden_size, num_layers=1)`\n",
    "  * `nn.LSTM(..)`\n",
    "  * `nn.GRU(..)`\n",
    "  * `nn.RNN(input_size, hidden_size, num_layers=1, bidirectional=True)`\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umO2AvmRn-76"
   },
   "source": [
    "To see how a multilayer RNN model can be built using one of these recurrent layers, in the following example, we create an RNN model with two recurrent layers of type RNN. Finally, we add a non-recurrent fully connected layer as the output layer, which returns a single output value as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDfq6mGjn-76"
   },
   "outputs": [],
   "source": [
    "## An example of building a RNN model\n",
    "## with simple RNN layer\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=2, \n",
    "                          batch_first=True)\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1) # output dimension is 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :] # take the output of last recurrent hidden layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = RNN(64, 32) \n",
    "\n",
    "print(model) \n",
    " \n",
    "model(torch.randn(10, 3, 64)) # Recall the input shape is (batch_size, sequence_length, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PICp1YMIn-77"
   },
   "source": [
    "### Building an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0l9RZdpn-77"
   },
   "source": [
    "Since we have very long sequences in the movie reviews, we use an LSTM layer to account for long-range effects. We create an RNN model for sentiment analysis, starting with an embedding layer producing word embeddings of feature size 20 (`embed_dim=20`).\n",
    "\n",
    "Then, a recurrent layer of type LSTM will be added. Finally, we add a fully connected layer as a hidden layer and another fully connected layer as the output layer, which returns a single class-membership probability value via the logistic sigmoid\n",
    "activation as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzGQI1oBn-77",
    "outputId": "7b024587-26aa-4f5c-ac54-0d7aa69debfd"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)  # checks for the first dimension being the batch size\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out) # note the cell output from LSTM\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC-MQF4Zn-78"
   },
   "source": [
    "Next we develop the `train` function to train the model on the given dataset for one epoch and return the classification accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq584UcXn-78"
   },
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBclQR8Zn-79"
   },
   "source": [
    "Similarly, we will develop the `evaluate` function to measure the model’s performance on a given dataset. For a binary classification with a single class-membership probability output, we use the binary cross-entropy loss (`BCELoss`) as the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoIiic1Ln-79"
   },
   "source": [
    "We train the model for 10 epochs and display the training and validation performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXbkjxrFn-79",
    "outputId": "e856f40c-0615-430b-b060-3d3441eb7ca1"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEQibTkSn-7-"
   },
   "source": [
    "After training this model for 10 epochs, we evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kq7xQVgkn-7-"
   },
   "outputs": [],
   "source": [
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdzIgYm7n-7-"
   },
   "source": [
    "Note that the above model is not the best when compared to the state-of-the-art methods used on the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgSjAaSDn-7_"
   },
   "source": [
    "#### The bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiRKYiZQn-7_"
   },
   "source": [
    "We can set the bidirectional configuration of the LSTM to `True`, which will make the recurrent layer pass through the input sequences from both directions, start to end, as well as in the reverse direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI8gB1jmn-7_"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        _, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzWXh25Hn-8A"
   },
   "source": [
    "The bidirectional RNN layer makes two passes over each input sequence: a forward pass and a reverse\n",
    "or backward pass (note that this is not to be confused with the forward and backward passes in the\n",
    "context of backpropagation). The resulting hidden states of these forward and backward passes are\n",
    "usually concatenated into a single hidden state. Other merge modes include summation, multiplication\n",
    "(multiplying the results of the two passes), and averaging (taking the average of the two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJHMoUTGn-8A"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "num_epochs = 5 \n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyhxlRY3n-8B"
   },
   "outputs": [],
   "source": [
    "test_dataset = IMDB(split='test')\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dKe1AXsn-8B"
   },
   "outputs": [],
   "source": [
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm-xhuN4n-8C"
   },
   "source": [
    "We can also try other types of recurrent layers, such as the regular RNN. However, as it turns out, a model\n",
    "built with regular recurrent layers won’t be able to reach a good predictive performance (even on the\n",
    "training data). For example, if you try replacing the bidirectional LSTM layer in the previous code with\n",
    "a unidirectional `nn.RNN` (instead of `nn.LSTM`) layer and train the model on full-length sequences, you\n",
    "may observe that the loss will not even decrease during training. The reason is that the sequences in\n",
    "this dataset are too long, so a model with an RNN layer cannot learn the long-term dependencies and\n",
    "may suffer from vanishing or exploding gradient problems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
