##  Lecture 7: Transformers in Practice

- Building large-scale language models by leveraging unlabeled data
  - Pre-training and fine-tuning transformer models
  - Leveraging unlabeled data with GPT
  - Using GPT-2 to generate new text
  - Bidirectional pre-training with BERT
  - The best of both worlds: BART
- Fine-tuning a BERT model in PyTorch
  - Loading the IMDb movie review dataset
  - Tokenizing the dataset
  - Loading and fine-tuning a pre-trained BERT model
  - Fine-tuning a transformer more conveniently using the Trainer API
